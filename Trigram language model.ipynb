{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93df2dac-9bc6-4cdd-a604-791cb2567bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kedha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\kedha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', '.', 's', '.']\n",
      "{('asian', 'exporters'): ['fear'], ('exporters', 'fear'): ['damage'], ('fear', 'damage'): ['from'], ('damage', 'from'): ['u'], ('from', 'u'): ['.'], ('u', '.'): ['s', 's'], ('.', 's'): ['.-', '.'], ('s', '.-'): ['japan'], ('.-', 'japan'): ['rift'], ('japan', 'rift'): ['mounting'], ('rift', 'mounting'): ['trade'], ('mounting', 'trade'): ['friction'], ('trade', 'friction'): ['between'], ('friction', 'between'): ['the'], ('between', 'the'): ['u'], ('the', 'u'): ['.']}\n",
      "asian exporters fear damage from u . s .- japan rift mounting trade friction between the u . s .- japan rift mounting trade friction between the u . s .- japan rift mounting trade friction between the u . s .- japan rift mounting trade friction between the u . s .- japan rift mounting trade friction between the\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Convert to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    return words\n",
    "\n",
    "def generate_trigrams(words):\n",
    "    trigrams = {}\n",
    "    for i in range(len(words) - 2):\n",
    "        pair = (words[i], words[i + 1])\n",
    "        follower = words[i + 2]\n",
    "        if pair in trigrams:\n",
    "            trigrams[pair].append(follower)\n",
    "        else:\n",
    "            trigrams[pair] = [follower]\n",
    "    return trigrams\n",
    "\n",
    "def generate_text(trigrams, max_length=60):\n",
    "    # Randomly select a starting pair\n",
    "    seed = random.choice(list(trigrams.keys()))\n",
    "    text = list(seed)\n",
    "    \n",
    "    # Generate text until reaching max_length or no more trigrams available\n",
    "    while len(text) < max_length:\n",
    "        current_pair = tuple(text[-2:])\n",
    "        if current_pair in trigrams:\n",
    "            next_word = random.choice(trigrams[current_pair])\n",
    "            text.append(next_word)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return ' '.join(text)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Load and preprocess the Reuters corpus from NLTK\n",
    "    reuters_words = reuters.words()\n",
    "    reuters_text = ' '.join(reuters_words[:20])  # Use only a subset of the corpus for example\n",
    "    \n",
    "    # Preprocess the text\n",
    "    words = preprocess_text(reuters_text)\n",
    "    print(words)\n",
    "    \n",
    "    # Generate trigrams\n",
    "    trigrams = generate_trigrams(words)\n",
    "    print(trigrams)\n",
    "    \n",
    "    # Generate text based on the trigram model\n",
    "    generated_text = generate_text(trigrams)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb76b08-5e64-48d2-9e60-26ef83cc6cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b49a596-ac2e-43c8-86e7-773696cf97fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kedha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\kedha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Convert to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    return words\n",
    "    \n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Load and preprocess the Reuters corpus from NLTK\n",
    "    reuters_words = reuters.words()\n",
    "    reuters_text = ' '.join(reuters_words[:10])  # Use only a subset of the corpus for example\n",
    "    \n",
    "    # Preprocess the text\n",
    "    words = preprocess_text(reuters_text)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904f8145-3914-4b5a-a9be-aa954f72c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kedha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\kedha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', '.', 's', '.']\n",
      "{('asian', 'exporters'): ['fear'], ('exporters', 'fear'): ['damage'], ('fear', 'damage'): ['from'], ('damage', 'from'): ['u'], ('from', 'u'): ['.'], ('u', '.'): ['s', 's'], ('.', 's'): ['.-', '.'], ('s', '.-'): ['japan'], ('.-', 'japan'): ['rift'], ('japan', 'rift'): ['mounting'], ('rift', 'mounting'): ['trade'], ('mounting', 'trade'): ['friction'], ('trade', 'friction'): ['between'], ('friction', 'between'): ['the'], ('between', 'the'): ['u'], ('the', 'u'): ['.']}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Convert to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    return words\n",
    "\n",
    "def generate_trigrams(words):\n",
    "    trigrams = {}\n",
    "    for i in range(len(words) - 2):\n",
    "        pair = (words[i], words[i + 1])\n",
    "        follower = words[i + 2]\n",
    "        if pair in trigrams:\n",
    "            trigrams[pair].append(follower)\n",
    "        else:\n",
    "            trigrams[pair] = [follower]\n",
    "    return trigrams\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Load and preprocess the Reuters corpus from NLTK\n",
    "    reuters_words = reuters.words()\n",
    "    reuters_text = ' '.join(reuters_words[:20])  # Use only a subset of the corpus for example\n",
    "    \n",
    "    # Preprocess the text\n",
    "    words = preprocess_text(reuters_text)\n",
    "    print(words)\n",
    "\n",
    "\n",
    "    \n",
    "    # Generate trigrams\n",
    "    trigrams = generate_trigrams(words)\n",
    "    print(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fc16c7-2b13-4943-9417-440b6ea0314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kedha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\kedha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', '.', 's', '.']\n",
      "{('asian', 'exporters'): ['fear'], ('exporters', 'fear'): ['damage'], ('fear', 'damage'): ['from'], ('damage', 'from'): ['u'], ('from', 'u'): ['.'], ('u', '.'): ['s', 's'], ('.', 's'): ['.-', '.'], ('s', '.-'): ['japan'], ('.-', 'japan'): ['rift'], ('japan', 'rift'): ['mounting'], ('rift', 'mounting'): ['trade'], ('mounting', 'trade'): ['friction'], ('trade', 'friction'): ['between'], ('friction', 'between'): ['the'], ('between', 'the'): ['u'], ('the', 'u'): ['.']}\n",
      "rift mounting trade friction between the u . s .- japan rift mounting trade friction between the u . s .- japan rift mounting trade friction between the u . s .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Convert to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    return words\n",
    "\n",
    "def generate_trigrams(words):\n",
    "    trigrams = {}\n",
    "    for i in range(len(words) - 2):\n",
    "        pair = (words[i], words[i + 1])\n",
    "        follower = words[i + 2]\n",
    "        if pair in trigrams:\n",
    "            trigrams[pair].append(follower)\n",
    "        else:\n",
    "            trigrams[pair] = [follower]\n",
    "    return trigrams\n",
    "\n",
    "def generate_text(trigrams, max_length=60):\n",
    "    # Randomly select a starting pair\n",
    "    seed = random.choice(list(trigrams.keys()))\n",
    "    text = list(seed)\n",
    "    \n",
    "    # Generate text until reaching max_length or no more trigrams available\n",
    "    while len(text) < max_length:\n",
    "        current_pair = tuple(text[-2:])\n",
    "        if current_pair in trigrams:\n",
    "            next_word = random.choice(trigrams[current_pair])\n",
    "            text.append(next_word)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return ' '.join(text)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Load and preprocess the Reuters corpus from NLTK\n",
    "    reuters_words = reuters.words()\n",
    "    reuters_text = ' '.join(reuters_words[:20])  # Use only a subset of the corpus for example\n",
    "    \n",
    "    # Preprocess the text\n",
    "    words = preprocess_text(reuters_text)\n",
    "    print(words)\n",
    "    \n",
    "    # Generate trigrams\n",
    "    trigrams = generate_trigrams(words)\n",
    "    print(trigrams)\n",
    "    \n",
    "    # Generate text based on the trigram model\n",
    "    generated_text = generate_text(trigrams)\n",
    "    print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
